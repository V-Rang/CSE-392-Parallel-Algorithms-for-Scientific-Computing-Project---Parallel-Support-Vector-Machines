{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Least Squares Support Vector Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "\n",
    "2. [Using the classifier](#using_classifier)\n",
    "\n",
    "    2.1 [CPU/Numpy version](#cpu_version)\n",
    "    \n",
    "    2.2 [GPU/PyTorch version](#gpu_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Least Squares Support Vector Machine (LSSVM) is a variation of the original Support Vector Machine (SVM) in which we have a slight change in the objective and restriction functions that results in a big simplification of the optimization problem.\n",
    "\n",
    "First, let's see the optimization problem of an SVM:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + C \\sum_{i=1}^{n} \\xi_i &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b)\\geq 1 - \\xi_i, && i = 1,..., n \\\\\n",
    "         && \\xi_i \\geq 0,                            && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, we have a set of inequality restrictions and when solving the optimization problem by it's dual we find a discriminative function, adding the kernel trick, of the type:\n",
    "\n",
    "\n",
    "$$ f(\\vec{x}) = sign \\ \\Big( \\sum_{i=1}^{n} \\alpha_i^o y_i K(\\vec{x}_i,\\vec{x}) + b_o \\Big) $$\n",
    "\n",
    "Where $\\alpha_i^o$ and $b_o$ denote optimum values. Giving enough regularization (smaller values of $C$) we get a lot of $\\alpha_i^o$ nulls, resulting in a sparse model in which we only need to save the pairs $(\\vec{x}_i,y_i)$ which have the optimum dual variable not null. The vectors $\\vec{x}_i$ with not null $\\alpha_i^o$ are known as support vectors (SV).\n",
    "\n",
    "\n",
    "\n",
    "In the LSSVM case, we change the inequality restrictions to equality restrictions. As the $\\xi_i$ may be negative we square its values in the objective function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{w},\\vec{\\xi})=\\frac{1}{2} \\vec{w}^T\\vec{w} + \\gamma \\frac{1}{2}\\sum_{i=1}^{n} \\xi_i^2 &&\\\\\n",
    "    s.t. && y_i(\\vec{w}^T\\vec{x}_i+b) = 1 - \\xi_i, && i = 1,..., n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The dual of this optimization problem results in a system of linear equations, a set of Karush-Khun-Tucker (KKT) equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    0 & \\vec{d}^T \\\\\n",
    "    \\vec{y} & \\Omega + \\gamma^{-1} I \n",
    "\\end{bmatrix}\n",
    "\\\n",
    "\\begin{bmatrix} \n",
    "    b  \\\\\n",
    "    \\vec{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    0 \\\\\n",
    "    \\vec{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where, with the kernel trick, &nbsp; $\\Omega_{i,j} = y_i y_j K(\\vec{x}_i,\\vec{x}_j)$,  &nbsp;  $\\vec{y} = [y_1 \\ y_2 \\ ... \\ y_n]^T$, &nbsp; $\\vec{\\alpha} = [\\alpha_1 \\ \\alpha_2 \\ ... \\ \\alpha_n]^T$ &nbsp;  e &nbsp; $\\vec{1} = [1 \\ 1 \\ ... \\ 1]^T$.\n",
    "\n",
    "The discriminative function of the LSSVM has the same form of the SVM but the $\\alpha_i^o$ aren't usually null, resulting in a bigger model. The big advantage of the LSSVM is in finding it's parameters, which is reduced to solving the linear system of the type:\n",
    "\n",
    "$$ A\\vec{z} = \\vec{b} $$\n",
    "\n",
    "A well-known solution of the linear system is when we minimize the square of the residues, that can be written as the optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    minimize && f_o(\\vec{z})=\\frac{1}{2}||A\\vec{z} - \\vec{b}||^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And have the analytical solution:\n",
    "\n",
    "$$ \\vec{z} = A^{\\dagger} \\vec{b} $$\n",
    "\n",
    "Where $A^{\\dagger}$ is the pseudo-inverse defined as:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using the classifier <a class=\"anchor\" id=\"using_classifier\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lssvm import LSSVC, LSSVC_GPU\n",
    "from utils.encoding import dummie2multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1078, 64)\n",
      "X_test.shape:  (719, 64)\n",
      "y_train.shape: (1078,)\n",
      "y_test.shape:  (719,)\n",
      "np.unique(y_train): [0 1 2 3 4 5 6 7 8 9]\n",
      "np.unique(y_test):  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Import digits recognition dataset (from sklearn)\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2020)\n",
    "\n",
    "# Scaling features (from sklearn)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_tr_norm = scaler.transform(X_train)\n",
    "X_ts_norm = scaler.transform(X_test)\n",
    "\n",
    "# Get information about input and outputs\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape:  {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape:  {y_test.shape}\")\n",
    "print(f\"np.unique(y_train): {np.unique(y_train)}\")\n",
    "print(f\"np.unique(y_test):  {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CPU/Numpy version <a class=\"anchor\" id=\"cpu_version\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian kernel:\n",
      "(1078, 1078)\n",
      "1079\n",
      "time =  0.2759546360030072\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'manual_pinv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaussian kernel:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m lssvc \u001b[38;5;241m=\u001b[39m LSSVC(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m) \u001b[38;5;66;03m# Class instantiation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mlssvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Fitting the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lssvc\u001b[38;5;241m.\u001b[39mpredict(X_ts_norm) \u001b[38;5;66;03m# Making predictions with the trained model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)) \u001b[38;5;66;03m# Calculate Accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/paralgos/project/lssvm/premade/LSSVM/lssvm/LSSVC.py:144\u001b[0m, in \u001b[0;36mLSSVC.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# converting to +1 for the desired class and -1 for all \u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# other classes\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     y_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    141\u001b[0m         (y_reshaped \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_labels[i])\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m         ,\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:,np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/paralgos/project/lssvm/premade/LSSVM/lssvm/LSSVC.py:97\u001b[0m, in \u001b[0;36mLSSVC._optimize_parameters\u001b[0;34m(self, X, y_values)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime = \u001b[39m\u001b[38;5;124m\"\u001b[39m, tim)\n\u001b[1;32m     96\u001b[0m tic1\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 97\u001b[0m CC\u001b[38;5;241m=\u001b[39m\u001b[43mmanual_pinv\u001b[49m(A)\n\u001b[1;32m     98\u001b[0m toc1\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     99\u001b[0m tim\u001b[38;5;241m=\u001b[39mtoc1\u001b[38;5;241m-\u001b[39mtic1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'manual_pinv' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the classifier with different kernels\n",
    "\n",
    "print('Gaussian kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='rbf', sigma=.5) # Class instantiation\n",
    "lssvc.fit(X_tr_norm, y_train) # Fitting the model\n",
    "y_pred = lssvc.predict(X_ts_norm) # Making predictions with the trained model\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)) # Calculate Accuracy\n",
    "print('acc_test = ', acc, '\\n')\n",
    "\n",
    "print('Polynomial kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='poly', d=2)\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print('acc_test = ', acc, '\\n')\n",
    "\n",
    "print('Linear kernel:')\n",
    "lssvc = LSSVC(gamma=1, kernel='linear')\n",
    "lssvc.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc.predict(X_ts_norm)\n",
    "acc = accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred))\n",
    "print('acc_test = ', acc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any questions about a specific method, the user can ask for help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "help(LSSVC.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also have an overview of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "help(LSSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also save the model in JSON format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lssvc.dump('model')\n",
    "loaded_model = LSSVC.load('model')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPU/PyTorch version <a class=\"anchor\" id=\"gpu_version\"></a>    \n",
    "\n",
    "It has the same functionalities and syntax of the CPU version, the difference is the use of PyTorch to run the operations on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Use the classifier with different kernels\n",
    "\n",
    "print('Gaussian kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='rbf', sigma=.5) \n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Polynomial kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='poly', d=2)\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')\n",
    "\n",
    "print('Linear kernel:')\n",
    "lssvc_gpu = LSSVC_GPU(gamma=1, kernel='linear')\n",
    "lssvc_gpu.fit(X_tr_norm, y_train)\n",
    "y_pred = lssvc_gpu.predict(X_ts_norm).cpu()\n",
    "print('acc_test = ', accuracy_score(dummie2multilabel(y_test), dummie2multilabel(y_pred)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also save the model in JSON format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lssvc_gpu.dump('model')\n",
    "loaded_model = LSSVC_GPU.load('model')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(loaded_model.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.json` is the same for the CPU and GPU version, giving the developer the possibility to train a model in GPU, dumping it in a .json, and loading in CPU version (the other way around is also possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lssvc.dump('model_from_cpu')\n",
    "lssvc_gpu = LSSVC_GPU.load('model_from_cpu')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lssvc_gpu.dump('model_from_gpu')\n",
    "lssvc = LSSVC.load('model_from_gpu')\n",
    "\n",
    "# Showing the same results\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc_gpu.predict(X_ts_norm).cpu())\n",
    "    )\n",
    ")\n",
    "print('acc_test = ', accuracy_score(\n",
    "        dummie2multilabel(y_test), \n",
    "        dummie2multilabel(lssvc.predict(X_ts_norm))\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
